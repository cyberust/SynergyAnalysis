{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIdAvzDTfYmq7/6PbCBKFf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyberust/SynergyAnalysis/blob/main/Synergy_Analysis_VC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y5DV6-pb0529"
      },
      "outputs": [],
      "source": [
        "# Google Driveをマウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install PyPDF2 python-docx spacy wordcloud matplotlib seaborn scikit-learn nltk\n",
        "\n",
        "# spaCyの英語モデルをダウンロード (初回のみ必要)\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "print(\"ステップ1完了: Driveのマウントとライブラリのインストールが完了しました。\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import PyPDF2\n",
        "from docx import Document as DocxDocument # ライブラリ名と区別するため別名\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# NLTKのストップワードリストをダウンロード (初回のみ必要)\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    nltk.download('punkt') # トークナイズに必要\n",
        "\n",
        "# spaCyの英語モデルをロード\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# 分析対象の人物リストとファイルパス\n",
        "# !!! ご自身のファイル構成に合わせてパスを修正してください !!!\n",
        "BASE_PATH = '/content/drive/MyDrive/' # Google DriveのCVファイルがあるフォルダパス\n",
        "\n",
        "# 人物名とファイル名のマッピング (実際のファイル名に合わせてください)\n",
        "# ファイル拡張子はテキスト抽出時に判断するため、ここでは不要\n",
        "# ただし、ファイル名にピリオドが多い場合は拡張子まで含めて正確に記載推奨\n",
        "# キーが人物名、バリューがファイル名（拡張子なし or あり）\n",
        "# 例: 'Arale Cohen': 'Arale_Cohen_CV.pdf'\n",
        "# 例: 'Yanay Geva': 'Yanay_Geva_Profile' (拡張子が.txtや.docxなどでもOK)\n",
        "persons_files = {\n",
        "    'Arale Cohen': 'arale_cohen_profile.txt',    # 例: PDFファイル\n",
        "    'Yanay Geva': 'yanay_geva_profile.txt',   # 例: DOCXファイル\n",
        "    'Yasuyuki Sakane': 'yasuyuki_sakane_profile.txt' # 例: TXTファイル\n",
        "}\n",
        "\n",
        "# 結果を格納する辞書\n",
        "analysis_results = {}\n",
        "\n",
        "print(\"ステップ2完了: ライブラリのインポートと基本設定が完了しました。\")"
      ],
      "metadata": {
        "id": "GztVlMEs1m5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def extract_text_from_txt(txt_file_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading TXT {txt_file_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def get_text_from_file(file_path):\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return \"\"\n",
        "\n",
        "    _, file_extension = os.path.splitext(file_path)\n",
        "    file_extension_lower = file_extension.lower()\n",
        "\n",
        "    if file_extension_lower == '.txt':\n",
        "        return extract_text_from_txt(file_path)\n",
        "    else:\n",
        "        print(f\"Unsupported file type: {file_extension_lower} for file {file_path}. This version only supports .txt files.\")\n",
        "        return \"\"\n",
        "\n",
        "# テキスト抽出の実行\n",
        "# !!! BASE_PATH と persons_files はステップ2で定義されている前提 !!!\n",
        "# 例:\n",
        "# BASE_PATH = '/content/drive/MyDrive/VC_Analysis_Data/'\n",
        "# persons_files = {\n",
        "#     'Arale Cohen': 'arale_cohen_profile.txt',\n",
        "#     'Yanay Geva': 'yanay_geva_profile.txt',\n",
        "#     'Yasuyuki Sakane': 'yasuyuki_sakane_profile.txt'\n",
        "# }\n",
        "\n",
        "raw_texts = {}\n",
        "for person, file_name in persons_files.items():\n",
        "    full_path = os.path.join(BASE_PATH, file_name)\n",
        "    print(f\"Processing {person}'s CV: {full_path}\")\n",
        "    raw_texts[person] = get_text_from_file(full_path)\n",
        "    if not raw_texts[person]:\n",
        "        print(f\"Could not extract text for {person}.\")\n",
        "    else:\n",
        "        print(f\"Successfully extracted text for {person} (length: {len(raw_texts[person])} chars).\")\n",
        "\n",
        "analysis_results = {} # analysis_resultsが未定義の場合に備えて初期化\n",
        "analysis_results['raw_texts'] = raw_texts\n",
        "print(\"\\nステップ3完了: ファイルからのテキスト抽出が試みられました。\")"
      ],
      "metadata": {
        "id": "3JUeWA4L2NnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_english = set(stopwords.words('english'))\n",
        "custom_stopwords_to_add = [\n",
        "    \"business\", \"market\", \"strategic\", \"japanese\", \"japan\", \"israeli\", \"development\",\n",
        "    \"investment\", \"line\", \"provide\", \"information\", \"target\", \"individual\",\n",
        "    \"experience\", \"year\", \"leadership\", \"company\", \"team\", \"project\", \"skill\",\n",
        "    \"ability\", \"knowledge\", \"strong\", \"also\", \"work\", \"role\", \"new\", \"growth\",\n",
        "    \"strategy\", \"management\", \"solution\", \"customer\", \"value\", \"global\", \"organization\"\n",
        "    # その他、出力を見ながら不要と思われる単語を追加\n",
        "]\n",
        "if 'stop_words_english' in globals():\n",
        "    stop_words_english.update(custom_stopwords_to_add)\n",
        "else:\n",
        "    from nltk.corpus import stopwords\n",
        "    stop_words_english = set(stopwords.words('english'))\n",
        "    stop_words_english.update(custom_stopwords_to_add)\n",
        "\n",
        "print(f\"Updated stop_words_english. New count: {len(stop_words_english)}\")\n",
        "\n",
        "import re # ステップ2でインポート済みのはず\n",
        "import spacy # ステップ2でインポート済みのはず\n",
        "# nlp = spacy.load('en_core_web_sm') # ステップ2でロード済みのはず\n",
        "\n",
        "def preprocess_text(text_original):\n",
        "    if not text_original:\n",
        "        return \"\"\n",
        "\n",
        "    text = text_original.lower()\n",
        "    text = re.sub(r'\\S*@\\S*\\s?', '', text)  # メールアドレス\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text) # URL\n",
        "    # text = re.sub(r'\\b\\d+\\b', '', text) # 数字のみの単語除去はtoken.is_digitで対応するのでコメントアウト\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.-]', '', text) # アルファベット(大小)、数字、空白、ドット、ハイフンを残す\n",
        "\n",
        "    doc = nlp(text) # グローバルスコープのnlpモデルを使用\n",
        "    lemmatized_tokens = []\n",
        "    for token in doc:\n",
        "        # 条件1: ストップワードでない\n",
        "        # 条件2: 句読点でない\n",
        "        # 条件3: 空白でない\n",
        "        # 条件4: 純粋な数字でない (token.is_digit)\n",
        "        # 条件5: レンマの長さが1より大きい (例: 2文字以上の単語を残す。必要に応じて >0 や >2 に変更)\n",
        "        # 条件6: レンマが空白文字のみでない\n",
        "        if not token.is_stop and \\\n",
        "           not token.is_punct and \\\n",
        "           not token.is_space and \\\n",
        "           not token.is_digit and \\\n",
        "           len(token.lemma_) > 1 and \\\n",
        "           not token.lemma_.isspace():\n",
        "            lemmatized_tokens.append(token.lemma_)\n",
        "\n",
        "    # デバッグ用に、処理後のトークンの一部を表示\n",
        "    # print(f\"--- Preprocessing Debug for preprocess_text ---\")\n",
        "    # print(f\"Original text (first 50): {text_original[:50]}\")\n",
        "    # print(f\"Text after regex (first 50): {text[:50]}\")\n",
        "    # print(f\"Lemmatized tokens (first 10): {lemmatized_tokens[:10]}\")\n",
        "\n",
        "    return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# 前処理の実行\n",
        "processed_texts = {}\n",
        "for person, text in raw_texts.items():\n",
        "    if text:\n",
        "        processed_texts[person] = preprocess_text(text)\n",
        "        print(f\"Processed text for {person} (length: {len(processed_texts[person])} chars).\")\n",
        "        # 前処理後のテキストの冒頭部分を表示 (確認用)\n",
        "        # print(f\"Preview for {person} (processed):\\n{processed_texts[person][:200]}...\\n\")\n",
        "    else:\n",
        "        processed_texts[person] = \"\"\n",
        "        print(f\"Skipping processing for {person} due to empty raw text.\")\n",
        "\n",
        "\n",
        "analysis_results['processed_texts'] = processed_texts\n",
        "print(\"\\nステップ4完了: テキスト前処理が完了しました。\")"
      ],
      "metadata": {
        "id": "b7rIdxUE4MwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter # これはステップ2でインポート済みのはずですが、念のため\n",
        "import matplotlib.pyplot as plt # ステップ2でインポート済みのはず\n",
        "import seaborn as sns # ステップ2でインポート済みのはず\n",
        "import pandas as pd # pandasをインポート\n",
        "import re # ステップ2でインポート済みのはず\n",
        "# spaCyやnltk関連もステップ2,4でロード・ダウンロード済みのはず\n",
        "\n",
        "# !!! スキルキーワードリスト (ご自身の関心に合わせて拡張・修正してください) !!!\n",
        "# 技術スキル、ビジネススキル、ソフトスキルなど\n",
        "SKILL_KEYWORDS = [\n",
        "    'python', 'java', 'c++', 'javascript', 'react', 'angular', 'vue', 'node.js',\n",
        "    'machine learning', 'deep learning', 'artificial intelligence', 'ai', 'nlp', 'natural language processing',\n",
        "    'data science', 'data analysis', 'big data', 'hadoop', 'spark',\n",
        "    'cloud computing', 'aws', 'azure', 'gcp',\n",
        "    'devops', 'docker', 'kubernetes',\n",
        "    'agile', 'scrum',\n",
        "    'project management', 'product management',\n",
        "    'business development', 'strategy', 'strategic planning', 'market analysis', 'financial modeling',\n",
        "    'vc', 'venture capital', 'private equity', 'investment', 'due diligence', 'fundraising',\n",
        "    'leadership', 'team management', 'communication', 'negotiation',\n",
        "    'blockchain', 'fintech', 'saas', 'b2b', 'b2c', 'iot',\n",
        "    'global expansion', 'market entry', # 日本市場関連のキーワード\n",
        "    # Yasuyuki Sakane氏の専門性に合わせて追加\n",
        "    'computer vision', 'robotics', 'cybersecurity', 'quantum computing',\n",
        "    # Arale Cohen氏、Yanay Geva氏の専門性に合わせて追加\n",
        "    'deal sourcing', 'portfolio management', 'exit strategy', 'term sheet'\n",
        "]\n",
        "\n",
        "def extract_skills(text, skill_keywords):\n",
        "    if not text:\n",
        "        return []\n",
        "\n",
        "    found_skills = set()\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    for skill in skill_keywords:\n",
        "        pattern = r'\\b' + re.escape(skill.lower()) + r'\\b'\n",
        "        if re.search(pattern, text_lower):\n",
        "            found_skills.add(skill)\n",
        "\n",
        "    return sorted(list(found_skills))\n",
        "\n",
        "# スキル抽出の実行\n",
        "# !!! processed_texts はステップ4で定義されている前提 !!!\n",
        "# 例:\n",
        "# processed_texts = {\n",
        "#     'Arale Cohen': \"processed text for arale...\",\n",
        "#     'Yanay Geva': \"processed text for yanay...\",\n",
        "#     'Yasuyuki Sakane': \"processed text for yasuyuki...\"\n",
        "# }\n",
        "# analysis_results = {'processed_texts': processed_texts} # analysis_results が存在し、processed_textsを含むこと\n",
        "\n",
        "extracted_skills = {}\n",
        "# processed_texts が analysis_results 内に存在するか確認\n",
        "if 'processed_texts' in analysis_results and isinstance(analysis_results['processed_texts'], dict):\n",
        "    current_processed_texts = analysis_results['processed_texts']\n",
        "else:\n",
        "    # analysis_resultsにprocessed_textsがない、または適切な型でない場合のフォールバック\n",
        "    # この場合、processed_textsがグローバルスコープに存在することを期待\n",
        "    if 'processed_texts' not in globals() or not isinstance(processed_texts, dict):\n",
        "        print(\"Error: 'processed_texts' is not defined or not a dictionary. Please ensure Step 4 has been run successfully.\")\n",
        "        current_processed_texts = {} # エラーを防ぐため空の辞書を割り当て\n",
        "    else:\n",
        "        current_processed_texts = processed_texts\n",
        "\n",
        "\n",
        "for person, text in current_processed_texts.items():\n",
        "    if text:\n",
        "        extracted_skills[person] = extract_skills(text, SKILL_KEYWORDS)\n",
        "        print(f\"Extracted skills for {person}: {extracted_skills[person]}\")\n",
        "    else:\n",
        "        extracted_skills[person] = []\n",
        "        print(f\"Skipping skill extraction for {person} due to empty processed text.\")\n",
        "\n",
        "# analysis_results に extracted_skills を格納\n",
        "if 'analysis_results' not in globals() or not isinstance(analysis_results, dict):\n",
        "    analysis_results = {} # analysis_resultsが未定義の場合、初期化\n",
        "analysis_results['extracted_skills'] = extracted_skills\n",
        "\n",
        "\n",
        "# スキルセットの比較と可視化\n",
        "all_skills_list = []\n",
        "for person_skills in extracted_skills.values():\n",
        "    all_skills_list.extend(person_skills)\n",
        "\n",
        "if not all_skills_list: # all_skills_listが空の場合の処理\n",
        "    print(\"No skills were extracted for any person. Skipping skill visualization.\")\n",
        "else:\n",
        "    skill_counts = Counter(all_skills_list)\n",
        "\n",
        "    # 全員のスキル保有状況を棒グラフで表示\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    # skill_countsが空でないことを確認してからDataFrameを作成\n",
        "    if skill_counts:\n",
        "        skill_df = pd.DataFrame(skill_counts.items(), columns=['Skill', 'Count']).sort_values('Count', ascending=False)\n",
        "        sns.barplot(x='Count', y='Skill', data=skill_df.head(20)) # 上位20スキル\n",
        "        plt.title('Top 20 Skills Found Across All CVs')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No skill counts to display in the bar graph.\")\n",
        "\n",
        "\n",
        "    # 各人のスキル数を比較\n",
        "    person_skill_counts = {person: len(skills) for person, skills in extracted_skills.items() if skills} # スキルがある人だけを対象\n",
        "    if person_skill_counts: # スキルを持つ人がいる場合のみプロット\n",
        "        plt.figure(figsize=(8, 5))\n",
        "        sns.barplot(x=list(person_skill_counts.keys()), y=list(person_skill_counts.values()))\n",
        "        plt.title('Number of Identified Skills per Person')\n",
        "        plt.ylabel('Number of Skills')\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No skills identified for any person to compare counts.\")\n",
        "\n",
        "\n",
        "    # 3者のスキル共通部分と独自部分 (簡易的な集合演算)\n",
        "    if len(extracted_skills) == 3: # 3人全員のデータがある場合\n",
        "        # extracted_skillsのキーの順序に依存しないように、明示的に名前でアクセスすることを推奨\n",
        "        # ただし、ここでは元のコードのロジックを維持\n",
        "        persons_names_list = list(extracted_skills.keys())\n",
        "        if len(persons_names_list) ==3 : # キーが3つあることを確認\n",
        "            skills_p1 = set(extracted_skills.get(persons_names_list[0], [])) # .getで見つからない場合のエラーを回避\n",
        "            skills_p2 = set(extracted_skills.get(persons_names_list[1], []))\n",
        "            skills_p3 = set(extracted_skills.get(persons_names_list[2], []))\n",
        "\n",
        "            common_skills_all = skills_p1.intersection(skills_p2, skills_p3)\n",
        "            print(f\"\\nSkills common to all three: {common_skills_all if common_skills_all else 'None'}\")\n",
        "\n",
        "            # Arale & Yanay の共通スキル\n",
        "            common_AY = skills_p1.intersection(skills_p2) - common_skills_all\n",
        "            print(f\"Skills common to {persons_names_list[0]} & {persons_names_list[1]} (but not {persons_names_list[2]}): {common_AY if common_AY else 'None'}\")\n",
        "\n",
        "            # Yasuyuki 独自のスキル (Arale, Yanayが持っていないもの)\n",
        "            unique_Y = skills_p3 - (skills_p1.union(skills_p2))\n",
        "            print(f\"Skills unique to {persons_names_list[2]} (compared to {persons_names_list[0]} & {persons_names_list[1]}): {unique_Y if unique_Y else 'None'}\")\n",
        "        else:\n",
        "            print(\"Could not perform 3-way skill comparison as data for exactly 3 persons is not available in extracted_skills.\")\n",
        "    elif extracted_skills: # データはあるが3人分ではない場合\n",
        "        print(\"\\nSkill comparison for common/unique skills is designed for exactly 3 persons. Adjust logic if needed for different number of persons.\")\n",
        "\n",
        "\n",
        "print(\"\\nステップ5完了: スキル抽出と可視化が試みられました。\")"
      ],
      "metadata": {
        "id": "q1vQPCmG4QEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ステップ6: トピックモデリング (LDA) - インデント修正・デバッグ強化版 ---\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# nltk.corpus.stopwords はステップ4でインポートとダウンロードがされている想定\n",
        "# from nltk.corpus import stopwords\n",
        "# stop_words_english = set(stopwords.words('english')) # ステップ4で定義されているはず\n",
        "\n",
        "# === 設定値 ===\n",
        "NUM_TOPICS = 3  # 各人物から抽出したいトピックの数 (テキスト内容が少ない場合は減らす)\n",
        "NUM_WORDS_PER_TOPIC = 7 # 各トピックを代表する単語の数\n",
        "MIN_WORDS_FOR_LDA = NUM_TOPICS * NUM_WORDS_PER_TOPIC * 2 # LDAを実行するための最低限の単語数（目安）\n",
        "\n",
        "# === 結果格納用の辞書 ===\n",
        "topic_models = {}\n",
        "topic_wordclouds = {}\n",
        "\n",
        "# === processed_texts の取得 ===\n",
        "# analysis_results 辞書と processed_texts がステップ4までに正しく準備されているか確認\n",
        "if 'analysis_results' in globals() and isinstance(analysis_results, dict) and \\\n",
        "   'processed_texts' in analysis_results and isinstance(analysis_results['processed_texts'], dict):\n",
        "    current_processed_texts = analysis_results['processed_texts']\n",
        "    print(\"Successfully loaded 'processed_texts' from 'analysis_results'.\")\n",
        "elif 'processed_texts' in globals() and isinstance(processed_texts, dict):\n",
        "    current_processed_texts = processed_texts\n",
        "    print(\"Warning: 'processed_texts' found in global scope, but not in 'analysis_results'. Make sure data flow is intended.\")\n",
        "    if 'analysis_results' not in globals() or not isinstance(analysis_results, dict):\n",
        "        analysis_results = {} # analysis_results がなければ初期化\n",
        "    analysis_results['processed_texts'] = current_processed_texts\n",
        "else:\n",
        "    print(\"CRITICAL ERROR: 'processed_texts' is not defined or not a dictionary. \\n\"\n",
        "          \"Please ensure Step 4 (Text Preprocessing) has been run successfully and \"\n",
        "          \"'processed_texts' dictionary is created and populated.\")\n",
        "    current_processed_texts = {}\n",
        "\n",
        "# === stop_words_english の取得 ===\n",
        "if 'stop_words_english' not in globals():\n",
        "    print(\"CRITICAL ERROR: 'stop_words_english' is not defined. \\n\"\n",
        "          \"Please ensure Step 4 (Text Preprocessing) has been run successfully, including nltk stopword download and definition.\")\n",
        "    try:\n",
        "        from nltk.corpus import stopwords\n",
        "        stop_words_english = set(stopwords.words('english'))\n",
        "        print(\"Fallback: Defined 'stop_words_english' in Step 6. Ideally, this should be done in Step 4.\")\n",
        "    except Exception as e_nltk:\n",
        "        print(f\"Fallback Error: Could not define 'stop_words_english': {e_nltk}\")\n",
        "        stop_words_english = set()\n",
        "else:\n",
        "    print(f\"Successfully loaded 'stop_words_english' (contains {len(stop_words_english)} words).\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Starting Topic Modeling (LDA) ---\")\n",
        "\n",
        "if not current_processed_texts:\n",
        "    print(\"No processed texts available to perform LDA. Skipping topic modeling.\")\n",
        "else:\n",
        "    for person, text_content in current_processed_texts.items():\n",
        "        print(f\"\\n--- Processing for: {person} ---\")\n",
        "\n",
        "        # 1. 入力テキストの検証とデバッグ出力\n",
        "        print(f\"  1. Verifying input text for {person}:\")\n",
        "        if not isinstance(text_content, str) or not text_content.strip():\n",
        "            print(f\"  !!! Issue: Processed text for {person} is empty, not a string, or whitespace only. Skipping LDA for this person.\")\n",
        "            print(f\"     Content: '{text_content}'\")\n",
        "            continue\n",
        "\n",
        "        print(f\"     First 300 characters of processed text: '{text_content[:300]}...'\")\n",
        "        words_in_text = text_content.split()\n",
        "        num_words = len(words_in_text)\n",
        "        print(f\"     Approximate word count: {num_words}\")\n",
        "\n",
        "        if num_words < MIN_WORDS_FOR_LDA:\n",
        "            print(f\"  !!! Issue: Text for {person} has only {num_words} words. \"\n",
        "                  f\"This might be too short for meaningful LDA with {NUM_TOPICS} topics. \"\n",
        "                  f\"(Minimum recommended: {MIN_WORDS_FOR_LDA} words). Skipping LDA for this person.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"  2. Attempting CountVectorizer for {person}:\")\n",
        "        try:\n",
        "            # === CountVectorizerのパラメータ設定 ===\n",
        "            # まずは最も単純な設定で試す (stop_wordsのみ)\n",
        "            # これでエラーが出る場合、stop_wordsリストか前処理自体に問題がある可能性が高い\n",
        "            vectorizer = CountVectorizer(\n",
        "                stop_words=list(stop_words_english)\n",
        "            )\n",
        "\n",
        "            # (次の試行案1：min_df, max_df, ngram_range を少し設定してみる)\n",
        "            # vectorizer = CountVectorizer(\n",
        "            #     stop_words=list(stop_words_english),\n",
        "            #     min_df=2,       # 最低2回出現する単語\n",
        "            #     max_df=0.95,    # 上位95%の頻出語は除外 (高頻度すぎる一般的単語対策)\n",
        "            #     ngram_range=(1, 2) # 1単語と2単語の組み合わせ\n",
        "            # )\n",
        "\n",
        "            # (次の試行案2：ストップワードなしで試す - 原因切り分け用)\n",
        "            # vectorizer = CountVectorizer(\n",
        "            #     stop_words=None # ストップワード除去を一時的に無効化\n",
        "            # )\n",
        "\n",
        "            document_term_matrix = vectorizer.fit_transform([text_content])\n",
        "            feature_names = vectorizer.get_feature_names_out()\n",
        "            num_features = len(feature_names)\n",
        "\n",
        "            print(f\"     Number of features (unique words) found by CountVectorizer: {num_features}\")\n",
        "            if num_features == 0:\n",
        "                print(f\"  !!! Issue: CountVectorizer found 0 features for {person}. \\n\"\n",
        "                      \"     This means no words remained after applying current CountVectorizer settings.\\n\"\n",
        "                      \"     Suggestions:\\n\"\n",
        "                      \"       - If using `stop_words`, ensure it's not removing everything. Try `stop_words=None` temporarily.\\n\"\n",
        "                      \"       - If `min_df` is high or `max_df` is low, adjust them.\\n\"\n",
        "                      \"       - Review Step 4 (Preprocessing) to ensure text is not overly cleaned (e.g., regex removing too much).\\n\"\n",
        "                      \"     Skipping LDA for this person.\")\n",
        "                # (デバッグ用) どの単語が残るか確認\n",
        "                # if text_content.strip(): # テキストが空でない場合のみ\n",
        "                #    temp_vec_no_filter = CountVectorizer()\n",
        "                #    temp_vec_no_filter.fit([text_content])\n",
        "                #    print(f\"     (Debug) Features without any CountVectorizer filters (except tokenization): {len(temp_vec_no_filter.get_feature_names_out())}\")\n",
        "                #    print(f\"     (Debug) Sample unfiltered features: {temp_vec_no_filter.get_feature_names_out()[:20]}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"     Sample features extracted by CountVectorizer: {feature_names[:min(20, num_features)]}\")\n",
        "\n",
        "\n",
        "            print(f\"  3. Attempting LDA for {person} with {num_features} features:\")\n",
        "\n",
        "            current_num_topics_for_person = NUM_TOPICS\n",
        "            if num_features < NUM_TOPICS:\n",
        "                print(f\"     Warning: Number of features ({num_features}) is less than NUM_TOPICS ({NUM_TOPICS}). \"\n",
        "                      f\"Adjusting NUM_TOPICS to {num_features} for this person.\")\n",
        "                current_num_topics_for_person = num_features\n",
        "\n",
        "            if current_num_topics_for_person == 0:\n",
        "                 print(f\"     Error: Cannot perform LDA as effective number of topics is 0 for {person}.\")\n",
        "                 continue\n",
        "\n",
        "            lda_model = LatentDirichletAllocation(\n",
        "                n_components=current_num_topics_for_person,\n",
        "                random_state=42,\n",
        "                learning_method='online',\n",
        "            )\n",
        "            # (次の試行案：LDAの事前分布パラメータ調整)\n",
        "            # lda_model = LatentDirichletAllocation(\n",
        "            #     n_components=current_num_topics_for_person,\n",
        "            #     random_state=42,\n",
        "            #     learning_method='online',\n",
        "            #     doc_topic_prior=0.1,  # alpha\n",
        "            #     topic_word_prior=0.01 # eta\n",
        "            # )\n",
        "            lda_model.fit(document_term_matrix)\n",
        "\n",
        "            person_topic_list = []\n",
        "            print(f\"     Top topics found for {person}:\")\n",
        "            for topic_idx, topic_distribution in enumerate(lda_model.components_):\n",
        "                if topic_distribution.sum() == 0:\n",
        "                    print(f\"     - Topic #{topic_idx + 1}: Empty distribution (no words).\")\n",
        "                    continue\n",
        "                top_word_indices = topic_distribution.argsort()[:-NUM_WORDS_PER_TOPIC - 1:-1]\n",
        "                top_words = [feature_names[i] for i in top_word_indices]\n",
        "                person_topic_list.append(top_words)\n",
        "                print(f\"     - Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n",
        "\n",
        "            topic_models[person] = person_topic_list\n",
        "\n",
        "            print(f\"  4. Generating Word Cloud for {person} topics:\")\n",
        "            all_topic_words_for_cloud = ' '.join([' '.join(words) for words in person_topic_list if words])\n",
        "            if all_topic_words_for_cloud.strip():\n",
        "                try:\n",
        "                    wordcloud_generator = WordCloud(\n",
        "                        width=800, height=400,\n",
        "                        background_color='white',\n",
        "                        stopwords=list(stop_words_english)\n",
        "                    ).generate(all_topic_words_for_cloud)\n",
        "\n",
        "                    topic_wordclouds[person] = wordcloud_generator\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.imshow(wordcloud_generator, interpolation='bilinear')\n",
        "                    plt.axis('off')\n",
        "                    plt.title(f'Topic Word Cloud for {person}')\n",
        "                    plt.show()\n",
        "                except ValueError as e_wc:\n",
        "                    print(f\"     !!! Issue: Could not generate word cloud for {person}. Error: {e_wc}\")\n",
        "                except Exception as e_wc_generic:\n",
        "                    print(f\"     !!! Issue: An unexpected error occurred during word cloud generation for {person}. Error: {e_wc_generic}\")\n",
        "            else:\n",
        "                print(f\"     Skipping word cloud for {person} as no valid topic words were found.\")\n",
        "\n",
        "        except ValueError as e_vec_lda:\n",
        "            print(f\"  !!! CRITICAL ERROR during LDA process for {person}: {e_vec_lda}\\n\"\n",
        "                  \"     This often means 'max_df corresponds to < documents than min_df' or similar issues \"\n",
        "                  \"     related to vocabulary size or document length after vectorization.\\n\"\n",
        "                  \"     Suggestions in the error message above and in the code comments should be checked.\")\n",
        "        except Exception as e_generic_person:\n",
        "            print(f\"  !!! An unexpected CRITICAL ERROR occurred while processing {person}: {e_generic_person}\")\n",
        "\n",
        "\n",
        "# === 結果の格納 ===\n",
        "if 'analysis_results' not in globals() or not isinstance(analysis_results, dict):\n",
        "    analysis_results = {}\n",
        "analysis_results['topic_models'] = topic_models\n",
        "\n",
        "print(\"\\n--- ステップ6完了: トピックモデリングが試みられました。 ---\")\n",
        "print(\"    詳細なデバッグ情報とエラーメッセージ（もしあれば）を上記で確認してください。\")"
      ],
      "metadata": {
        "id": "e6epnTR-4u-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 有効な処理済みテキストのリストと対応する人物名リストを作成\n",
        "persons_for_similarity = [p for p, t in processed_texts.items() if t]\n",
        "texts_for_similarity = [processed_texts[p] for p in persons_for_similarity if processed_texts[p]]\n",
        "\n",
        "if len(texts_for_similarity) < 2:\n",
        "    print(\"Not enough valid texts (need at least 2) to perform similarity analysis. Skipping.\")\n",
        "else:\n",
        "    print(\"\\nPerforming Text Similarity Analysis...\")\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(texts_for_similarity)\n",
        "\n",
        "        # コサイン類似度を計算\n",
        "        cosine_sim_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "        # 結果をDataFrameで表示\n",
        "        similarity_df = pd.DataFrame(cosine_sim_matrix, index=persons_for_similarity, columns=persons_for_similarity)\n",
        "        analysis_results['similarity_matrix'] = similarity_df\n",
        "\n",
        "        print(\"Cosine Similarity Matrix:\")\n",
        "        print(similarity_df)\n",
        "\n",
        "        # ヒートマップで可視化\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.heatmap(similarity_df, annot=True, cmap=\"YlGnBu\", fmt=\".2f\")\n",
        "        plt.title('CV Text Similarity (Cosine Similarity on TF-IDF)')\n",
        "        plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during similarity analysis: {e}\")\n",
        "\n",
        "print(\"\\nステップ7完了: テキスト類似度分析が試みられました。\")"
      ],
      "metadata": {
        "id": "UPHZqFFBC7HP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\\n--- 統合分析レポート ---\")\n",
        "print(\"========================\\n\")\n",
        "\n",
        "# 0. 基本情報\n",
        "print(\"## 0. 分析対象\")\n",
        "for person in persons_files.keys():\n",
        "    print(f\"- {person}\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 1. スキル分析サマリー\n",
        "print(\"## 1. スキル分析概要\")\n",
        "if 'extracted_skills' in analysis_results:\n",
        "    for person, skills in analysis_results['extracted_skills'].items():\n",
        "        print(f\"\\n### {person}:\")\n",
        "        if skills:\n",
        "            print(f\"- 主な確認スキル ({len(skills)}個): {', '.join(skills[:15])}{'...' if len(skills) > 15 else ''}\")\n",
        "        else:\n",
        "            print(\"- 確認された特定のスキルはありません\")\n",
        "\n",
        "    # スキル補完性に関するコメント (ステップ5の結果を基に)\n",
        "    skills_data = analysis_results.get('extracted_skills', {})\n",
        "    if len(skills_data) == 3: # 3人全員のデータがあることを想定\n",
        "        p_names = list(skills_data.keys()) # 人物名のリストを取得\n",
        "        # 人物名が 'Arale Cohen', 'Yanay Geva', 'Yasuyuki Sakane' の順であることを期待するが、\n",
        "        # より堅牢にするには、persons_filesのキーの順序に依存しない方法で人物を特定する\n",
        "        # ここでは、キーの順序に依存する形で実装（もし順序が異なる場合は要調整）\n",
        "        name_arale = p_names[0]\n",
        "        name_yanay = p_names[1]\n",
        "        name_yasuyuki = p_names[2]\n",
        "\n",
        "        s_arale = set(skills_data.get(name_arale, []))\n",
        "        s_yanay = set(skills_data.get(name_yanay, []))\n",
        "        s_yasuyuki = set(skills_data.get(name_yasuyuki, []))\n",
        "\n",
        "\n",
        "        common_all = s_arale.intersection(s_yanay, s_yasuyuki)\n",
        "        unique_yasuyuki = s_yasuyuki - (s_arale.union(s_yanay))\n",
        "\n",
        "        print(\"\\n### スキルの相互補完性 (概念的):\")\n",
        "        if common_all:\n",
        "            print(f\"- 3者共通スキル: {', '.join(list(common_all)[:5])}{'...' if len(common_all) > 5 else ''}\")\n",
        "        if unique_yasuyuki:\n",
        "            print(f\"- Yasuyuki Sakane氏のユニークスキル ({name_arale}氏, {name_yanay}氏との比較): {', '.join(list(unique_yasuyuki)[:5])}{'...' if len(unique_yasuyuki) > 5 else ''}\")\n",
        "            print(\"  - これらのユニークスキルは、チームの技術的専門性を拡張し、新しい事業領域の探索に貢献する可能性があります。\")\n",
        "\n",
        "        # Arale & Yanay が持ち、Yasuyukiが持たないスキル\n",
        "        AY_only_skills = (s_arale.union(s_yanay)) - s_yasuyuki\n",
        "        if AY_only_skills:\n",
        "             print(f\"- {name_arale}氏と{name_yanay}氏の ({name_yasuyuki}氏との比較) 主な保有スキル: {', '.join(list(AY_only_skills)[:5])}{'...' if len(AY_only_skills) > 5 else ''}\")\n",
        "             print(\"  - これらはVC運営および投資関連のコアコンピタンスである可能性が高く、Yasuyuki氏の技術専門性と結合することでシナジーが期待されます。\")\n",
        "else:\n",
        "    print(\"スキル分析結果がありません。\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# 2. トピックモデリングサマリー\n",
        "print(\"## 2. 主要トピック (専門分野) 概要\")\n",
        "if 'topic_models' in analysis_results and analysis_results['topic_models']:\n",
        "    for person, topics in analysis_results['topic_models'].items():\n",
        "        print(f\"\\n### {person}:\")\n",
        "        if topics:\n",
        "            for i, topic_words in enumerate(topics):\n",
        "                print(f\"- トピック {i+1}: {', '.join(topic_words)}\")\n",
        "            print(f\"  - {person}氏のCVは、主に上記のようなトピックに関連する経験/専門性を含んでいる可能性が高いです。\")\n",
        "        else:\n",
        "            print(\"- 主要なトピックを抽出できませんでした。\")\n",
        "else:\n",
        "    print(\"トピックモデリングの結果がありません。\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# 3. テキスト類似度サマリー\n",
        "print(\"## 3. CV内容の類似度分析概要\")\n",
        "if 'similarity_matrix' in analysis_results:\n",
        "    sim_matrix = analysis_results['similarity_matrix']\n",
        "    print(\"以下は、各人物のCV内容間のコサイン類似度行列です (1に近いほど類似度が高いことを示します):\")\n",
        "    print(sim_matrix)\n",
        "\n",
        "    persons_list = list(sim_matrix.columns)\n",
        "    if len(persons_list) == 3: # 3人の場合\n",
        "        # persons_listの順序が 'Arale Cohen', 'Yanay Geva', 'Yasuyuki Sakane' であると仮定\n",
        "        # persons_filesのキーの順序と一致することを期待\n",
        "        p1_name = persons_list[0]\n",
        "        p2_name = persons_list[1]\n",
        "        p3_name = persons_list[2]\n",
        "\n",
        "        # AraleとYanayの類似度\n",
        "        sim_AY = sim_matrix.loc[p1_name, p2_name]\n",
        "        # AraleとYasuyukiの類似度\n",
        "        sim_AYasu = sim_matrix.loc[p1_name, p3_name]\n",
        "        # YanayとYasuyukiの類似度\n",
        "        sim_YYasu = sim_matrix.loc[p2_name, p3_name]\n",
        "\n",
        "        print(f\"\\n- {p1_name}氏と{p2_name}氏のCV類似度: {sim_AY:.2f}\")\n",
        "        print(f\"- {p1_name}氏と{p3_name}氏のCV類似度: {sim_AYasu:.2f}\")\n",
        "        print(f\"- {p2_name}氏と{p3_name}氏のCV類似度: {sim_YYasu:.2f}\")\n",
        "\n",
        "        if sim_AY > sim_AYasu and sim_AY > sim_YYasu:\n",
        "            print(f\"  - {p1_name}氏と{p2_name}氏は、{p3_name}氏に比べて相対的に類似した経験/経歴背景を持つ可能性があります (既存VC創業者としての共通点など)。\")\n",
        "        elif sim_AYasu > sim_AY and sim_YYasu > sim_AY : # 修正: Yasuyuki氏と他の2人のどちらかとの類似性が高い場合\n",
        "             # この条件は少し曖昧なので、個別の比較でコメントする方が良いかもしれません\n",
        "             print(f\"  - {p3_name}氏は、{p1_name}氏または{p2_name}氏のどちらかと、一定の類似した経験/経歴背景を持つ可能性があります。\")\n",
        "\n",
        "\n",
        "        avg_sim_yasu_vs_founders = (sim_AYasu + sim_YYasu) / 2\n",
        "        print(f\"  - {p3_name}氏と既存創業者({p1_name}氏, {p2_name}氏)間の平均CV類似度: {avg_sim_yasu_vs_founders:.2f}\")\n",
        "        print(\"  - この類似度は、履歴書に記述された内容の言語的表現、プロジェクトの種類、使用された用語などの共通点を反映する可能性があります。\")\n",
        "\n",
        "else:\n",
        "    print(\"テキスト類似度分析の結果がありません。\")\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "# 4. 3者間の相関性、共起関係、シナジー予測 (概念的分析)\n",
        "print(\"## 4. 3者間の相互作用、共創シナジー、成長加速に関する予測 (概念的分析)\")\n",
        "print(\"\"\"\n",
        "Yasuyuki Sakane氏の参画は、Arale Cohen氏とYanay Geva氏のVCファームに以下のような多面的なポジティブな影響をもたらすと予測されます。\n",
        "\n",
        "### A. 技術的専門性の飛躍的向上とデューデリジェンス強化:\n",
        "- Yasuyuki氏の深い技術的知見（例：AI、特定技術ドメイン）は、投資候補企業の技術デューデリジェンスの精度と深度を大幅に向上させます。\n",
        "- 特に、日本市場における高度な技術を持つスタートアップの発掘、評価、および技術的課題の特定において、比類なき能力を発揮します。\n",
        "- 投資後のポートフォリオ企業に対し、具体的な技術アドバイスやアーキテクチャレビュー、開発戦略支援などを提供し、企業価値向上に直接的に貢献します。\n",
        "\n",
        "### B. 事業拡大と戦略的オプションの多様化:\n",
        "- **日本市場への本格参入加速:** Yasuyuki氏の持つ日本市場や技術コミュニティへの深い理解とネットワークは、日本市場への参入戦略を具体化し、実行を加速させる上で不可欠な要素となります。現地パートナーシップ構築、ローカライズ戦略、日本特有のビジネス慣行への対応などが円滑に進むことが期待されます。\n",
        "- **新規技術領域への投資拡大:** Yasuyuki氏の専門分野（例：AI, ブロックチェーン等）への知見を活用し、VCファームとして新たな成長領域への投資を戦略的に拡大できます。これは、ファームの専門性を高め、競争優位性を確立する上で重要です。\n",
        "- **イノベーションの促進:** Arale氏とYanay氏の持つ広範なビジネス経験や投資実績と、Yasuyuki氏の最先端技術への洞察が融合することで、従来の発想にとらわれない革新的なビジネスモデルや投資テーマの創出が期待されます。\n",
        "\n",
        "### C. Arale Cohen氏とYanay Geva氏の関係性の変化・変革・成長:\n",
        "- **意思決定プロセスの質の向上:** Yasuyuki氏からの技術的視点に基づく客観的なインプットは、Arale氏とYanay氏の議論をより多角的で深いものにし、最終的な投資判断や戦略決定の質を向上させます。特に技術リスクの評価において重要な役割を果たします。\n",
        "- **役割分担の最適化と効率化:** Yasuyuki氏が技術関連の評価や目利き、ポートフォリオ企業の技術支援といった専門領域を担当することで、Arale氏とYanay氏は、ファンドレイジング、LPリレーション、大規模な事業戦略、ネットワーキングといった、それぞれの強みを活かせる領域により一層注力できるようになります。\n",
        "- **相互学習と視点の拡張による成長:** Yasuyuki氏の異なるバックグラウンド、専門知識、そして日本という異なる文化圏からの視点は、Arale氏とYanay氏に新たな気づきや学びをもたらし、両者の視野を広げ、リーダーとしての成長を促進します。これにより、チーム全体の適応力と問題解決能力が向上します。\n",
        "- **触媒としての役割:** Yasuyuki氏の参画は、時にArale氏とYanay氏の間で生じる可能性のある意見の相違や膠着状態に対し、新たな視点や解決策を提示する「触媒」としての役割を果たし、より建設的な議論と強固なパートナーシップへと導く可能性があります。\n",
        "\n",
        "### D. 複利的な成長とビジネススケール拡大のステップ（予測）:\n",
        "1.  **フェーズ1: 基盤構築と初期シナジー（〜1年）**\n",
        "    *   Yasuyuki氏による技術評価プロセスの導入・標準化。既存・新規ディールの技術DDへの全面的な関与。\n",
        "    *   日本市場における初期的なネットワーク構築と情報収集、有望な技術シーズの特定。\n",
        "    *   VCファーム内での技術トレンド勉強会などを通じた知識共有とチーム全体の技術リテラシー向上。\n",
        "    *   Arale氏・Yanay氏との信頼関係構築と、3者の役割・責任範囲の明確化。\n",
        "2.  **フェーズ2: 戦略実行と成果の可視化（1〜3年）**\n",
        "    *   日本市場における具体的な投資案件の実行、または日本企業との戦略的提携の実現。\n",
        "    *   Yasuyuki氏の専門性を活かした特定技術分野特化型の小規模ファンド（または投資枠）の検討・立ち上げ。\n",
        "    *   ポートフォリオ企業への技術支援による成功事例の創出（例：製品開発サイクルの短縮、技術的課題の解決）。\n",
        "    *   3者の連携によるディールソーシングチャネルの拡大と質の向上。\n",
        "3.  **フェーズ3: スケール拡大と持続的成長（3年〜）**\n",
        "    *   日本市場での成功を足がかりとしたアジア他地域への展開可能性の検討。\n",
        "    *   VCファームが「技術に強い投資ファーム」としての確固たるブランドを確立。\n",
        "    *   Yasuyuki氏がリーダーシップを発揮し、技術チームの育成や次世代の専門家採用を推進。\n",
        "    *   継続的なイノベーションと相互補完による複利的な成長モデルを確立し、ファンドパフォーマンスの最大化とVCファームの持続的なスケール拡大を実現。\n",
        "\n",
        "### E. 成功のための重要因子:\n",
        "- **明確なビジョン共有:** 3者がVCファームの将来像とYasuyuki氏の役割について共通認識を持つこと。\n",
        "- **オープンなコミュニケーション:** 定期的な情報共有、率直な意見交換、建設的なフィードバックが不可欠。\n",
        "- **文化の融合と相互尊重:** 異なるバックグラウンドを持つメンバー間の文化的な違いを理解し、尊重し合う組織文化の醸成。\n",
        "- **柔軟な役割分担と権限委譲:** 状況の変化に応じて役割を柔軟に見直し、Yasuyuki氏の専門性を最大限に活かせるような適切な権限委譲。\n",
        "- **成果の評価とインセンティブ:** Yasuyuki氏の貢献を適切に評価し、モチベーションを維持するためのインセンティブ設計。\n",
        "\"\"\")\n",
        "\n",
        "print(\"========================\")\n",
        "print(\"--- 分析レポート終了 ---\")"
      ],
      "metadata": {
        "id": "LMe0u279DA3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ステップ7: 共起関係分析 (Co-occurrence Network) ---\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import networkx as nx # networkx をインポート\n",
        "# matplotlib.pyplot は既にインポートされているはず (pltとして)\n",
        "\n",
        "# ステップ7の冒頭、tokens を作成する前に追加\n",
        "# stop_words_english はステップ6などで定義・更新されている想定\n",
        "\n",
        "#text_for_network = current_processed_texts[target_person_for_network]\n",
        "#raw_tokens = text_for_network.split()\n",
        "#tokens = [token for token in raw_tokens if token not in stop_words_english and len(token) > 1] # ストップワード除去と短い単語除去\n",
        "\n",
        "print(\"\\n--- ステップ7: 共起関係分析 ---\")\n",
        "\n",
        "# 分析対象の人物を選択 (ここでは例として Yanay Geva)\n",
        "# 全員分実行する場合はループ処理にする\n",
        "target_person_for_network = 'Yasuyuki Sakane' # または他の人物名\n",
        "\n",
        "if target_person_for_network in current_processed_texts and current_processed_texts[target_person_for_network]:\n",
        "    text_for_network = current_processed_texts[target_person_for_network]\n",
        "    tokens = text_for_network.split() # 簡単のためスペースで単語分割\n",
        "\n",
        "    # 共起をカウントするためのウィンドウサイズ (例: 同じ文脈と見なす単語の距離)\n",
        "    window_size = 5\n",
        "    # 最低共起回数 (これ以下のペアはネットワークに表示しない)\n",
        "    min_cooccurrence = 10 # テキスト量や見たい粒度で調整\n",
        "\n",
        "    co_occurrences = Counter()\n",
        "\n",
        "    # ウィンドウをスライドさせながら共起ペアをカウント\n",
        "    for i in range(len(tokens) - window_size + 1):\n",
        "        window = sorted(list(set(tokens[i : i + window_size]))) # ウィンドウ内のユニークな単語 (ソートしてペアの順序を固定)\n",
        "        for w1, w2 in itertools.combinations(window, 2): # ウィンドウ内の全ての単語ペア\n",
        "            if w1 != w2: # 同じ単語同士のペアは除く (itertools.combinationsなら不要だが念のため)\n",
        "                # 単語のペアをアルファベット順にしてカウント (例: (apple, banana) と (banana, apple) を同じと見なす)\n",
        "                pair = tuple(sorted((w1, w2)))\n",
        "                co_occurrences[pair] += 1\n",
        "\n",
        "    # 共起回数がmin_cooccurrence以上のペアのみを抽出\n",
        "    significant_co_occurrences = {pair: count for pair, count in co_occurrences.items() if count >= min_cooccurrence}\n",
        "\n",
        "    if not significant_co_occurrences:\n",
        "        print(f\"No significant co-occurrences found for {target_person_for_network} with min_cooccurrence={min_cooccurrence}.\")\n",
        "    else:\n",
        "        print(f\"Found {len(significant_co_occurrences)} significant co-occurrence pairs for {target_person_for_network}.\")\n",
        "\n",
        "        # NetworkXグラフの作成\n",
        "        G = nx.Graph()\n",
        "        for pair, weight in significant_co_occurrences.items():\n",
        "            G.add_edge(pair[0], pair[1], weight=weight)\n",
        "\n",
        "        if not G.nodes():\n",
        "            print(f\"Graph for {target_person_for_network} has no nodes. Cannot draw.\")\n",
        "        else:\n",
        "            plt.figure(figsize=(25, 25))\n",
        "            pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42) # ノードの配置アルゴリズム\n",
        "\n",
        "            # ノードのサイズを次数（接続しているエッジの数）に応じて変更\n",
        "            node_sizes = [G.degree(node) * 50 for node in G.nodes()]\n",
        "\n",
        "            # エッジの太さを共起回数（重み）に応じて変更\n",
        "            edge_widths = [G.edges[edge]['weight'] * 0.5 for edge in G.edges()]\n",
        "\n",
        "            nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.8)\n",
        "            nx.draw_networkx_edges(G, pos, width=edge_widths, edge_color='gray', alpha=0.5)\n",
        "            nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "\n",
        "            plt.title(f'Co-occurrence Network for {target_person_for_network} (min_cooccurrence={min_cooccurrence})', size=15)\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            # (オプション) 特に中心性の高い単語を表示\n",
        "            # try:\n",
        "            #     degree_centrality = nx.degree_centrality(G)\n",
        "            #     sorted_degree = sorted(degree_centrality.items(), key=lambda item: item[1], reverse=True)\n",
        "            #     print(f\"\\nTop 10 words by Degree Centrality for {target_person_for_network}:\")\n",
        "            #     for i, (word, centrality) in enumerate(sorted_degree[:10]):\n",
        "            #         print(f\"{i+1}. {word}: {centrality:.3f}\")\n",
        "            # except Exception as e_centrality:\n",
        "            #     print(f\"Could not calculate centrality: {e_centrality}\")\n",
        "\n",
        "else:\n",
        "    print(f\"No processed text found for {target_person_for_network} to perform co-occurrence analysis.\")\n",
        "\n",
        "print(\"\\n--- ステップ7完了: 共起関係分析が試みられました。 ---\")"
      ],
      "metadata": {
        "id": "Od3RHX05DnTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOazF3OXDzV_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}